{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f214ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f348430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bba799",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./license_data_with_missing_parts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243aa752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "label_set = {\"O\"}\n",
    "\n",
    "for item in data:\n",
    "    text = item[\"text\"]\n",
    "    entities = item[\"entities\"]\n",
    "\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc]\n",
    "    token_start_char_idxs = [token.idx for token in doc]\n",
    "\n",
    "    label_seq = [\"O\"] * len(words)\n",
    "\n",
    "    for entity in entities:\n",
    "        ent_start = entity[\"start\"]\n",
    "        ent_end = entity[\"end\"]\n",
    "        ent_label = entity[\"label\"]\n",
    "\n",
    "        for i, token in enumerate(doc):\n",
    "            token_start = token.idx\n",
    "            token_end = token.idx + len(token)\n",
    "\n",
    "            if token_start >= ent_start and token_end <= ent_end:\n",
    "                if token_start == ent_start:\n",
    "                    label_seq[i] = f\"B-{ent_label}\"\n",
    "                else:\n",
    "                    label_seq[i] = f\"I-{ent_label}\"\n",
    "                label_set.add(label_seq[i])\n",
    "\n",
    "    sentences.append(words)\n",
    "    labels.append(label_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57decac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875324ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = {word for sent in sentences for word in sent}\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(word_set)}\n",
    "word2idx[\"PAD\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324de9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {label: idx for idx, label in enumerate(label_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071612c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word2idx[word] for word in sent] for sent in sentences]\n",
    "y = [[label2idx[label] for label in label_seq] for label_seq in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258eaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(sent) for sent in X)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=max_length, padding=\"post\")\n",
    "y = pad_sequences(y, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([to_categorical(seq, num_classes=len(label2idx)) for seq in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8350fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "embedding_file = \"glove.6B.200d.txt\"\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee12232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word2idx), output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix], input_length=max_length,\n",
    "              trainable=False),\n",
    "    Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.5, recurrent_dropout=0.2)),\n",
    "    Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.5, recurrent_dropout=0.2)),\n",
    "    Bidirectional(LSTM(units=32, return_sequences=True, dropout=0.5, recurrent_dropout=0.2)),\n",
    "    Dense(len(label2idx), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=35, validation_data=(X_test, y_test))\n",
    "\n",
    "model.save(\"ner_model.h5\")\n",
    "\n",
    "model.save(\"ner_model.keras\", save_format=\"keras\")\n",
    "\n",
    "model.export(\"ner_model_savedmodel\")\n",
    "\n",
    "with open(\"word2idx.json\", \"w\") as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "with open(\"label2idx.json\", \"w\") as f:\n",
    "    json.dump(label2idx, f)\n",
    "\n",
    "print(\"Model training complete. Saved as 'ner_model.h5'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
