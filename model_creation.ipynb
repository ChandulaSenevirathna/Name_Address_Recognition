{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f214ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f348430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bba799",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./license_data_with_missing_parts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243aa752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "label_set = {\"O\"}\n",
    "\n",
    "for item in data:\n",
    "    text = item[\"text\"]\n",
    "    entities = item[\"entities\"]\n",
    "\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc]\n",
    "    token_start_char_idxs = [token.idx for token in doc]\n",
    "\n",
    "    label_seq = [\"O\"] * len(words)\n",
    "\n",
    "    for entity in entities:\n",
    "        ent_start = entity[\"start\"]\n",
    "        ent_end = entity[\"end\"]\n",
    "        ent_label = entity[\"label\"]\n",
    "\n",
    "        for i, token in enumerate(doc):\n",
    "            token_start = token.idx\n",
    "            token_end = token.idx + len(token)\n",
    "\n",
    "            if token_start >= ent_start and token_end <= ent_end:\n",
    "                if token_start == ent_start:\n",
    "                    label_seq[i] = f\"B-{ent_label}\"\n",
    "                else:\n",
    "                    label_seq[i] = f\"I-{ent_label}\"\n",
    "                label_set.add(label_seq[i])\n",
    "\n",
    "    sentences.append(words)\n",
    "    labels.append(label_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57decac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875324ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = {word for sent in sentences for word in sent}\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(word_set)}\n",
    "word2idx[\"PAD\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324de9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {label: idx for idx, label in enumerate(label_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071612c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[word2idx[word] for word in sent] for sent in sentences]\n",
    "y = [[label2idx[label] for label in label_seq] for label_seq in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258eaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(sent) for sent in X)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=max_length, padding=\"post\")\n",
    "y = pad_sequences(y, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([to_categorical(seq, num_classes=len(label2idx)) for seq in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8350fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "embedding_file = \"glove.6B.200d.txt\"\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee12232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word2idx), output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix], input_length=max_length,\n",
    "              trainable=False),\n",
    "    Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.5, recurrent_dropout=0.2)),\n",
    "    Bidirectional(LSTM(units=64, return_sequences=True, dropout=0.5, recurrent_dropout=0.2)),\n",
    "    Bidirectional(LSTM(units=32, return_sequences=True, dropout=0.5, recurrent_dropout=0.2)),\n",
    "    Dense(len(label2idx), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=35, validation_data=(X_test, y_test))\n",
    "\n",
    "model.save(\"ner_model.h5\")\n",
    "\n",
    "model.save(\"ner_model.keras\", save_format=\"keras\")\n",
    "\n",
    "model.export(\"ner_model_savedmodel\")\n",
    "\n",
    "with open(\"word2idx.json\", \"w\") as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "with open(\"label2idx.json\", \"w\") as f:\n",
    "    json.dump(label2idx, f)\n",
    "\n",
    "print(\"Model training complete. Saved as 'ner_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r ner_model_savedmodel.zip ner_model_savedmodel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "with open(\"word2idx.json\", \"r\") as f:\n",
    "    word2idx = json.load(f)\n",
    "\n",
    "with open(\"label2idx.json\", \"r\") as f:\n",
    "    label2idx = json.load(f)\n",
    "\n",
    "idx2label = {int(v): k for k, v in label2idx.items()}\n",
    "\n",
    "model = load_model(\"ner_model.h5\")\n",
    "\n",
    "\n",
    "max_length = 0\n",
    "for word in word2idx:\n",
    "    if word != \"PAD\":\n",
    "        max_length = max(max_length, len(word))\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "def predict_entities_dict(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc]\n",
    "\n",
    "    X_test = pad_sequences([[word2idx.get(w, 0) for w in words]], maxlen=max_length, padding=\"post\")\n",
    "\n",
    "    predictions = model.predict(X_test)[0]\n",
    "    predicted_labels = [idx2label[np.argmax(p)] for p in predictions[:len(words)]]\n",
    "\n",
    "    entity_dict = {}\n",
    "    current_entity = []\n",
    "    current_entity_type = None\n",
    "\n",
    "    for word, label in zip(words, predicted_labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entity_dict[current_entity_type] = \" \".join(current_entity)\n",
    "            current_entity_type = label[2:]\n",
    "            current_entity = [word]\n",
    "        elif label.startswith(\"I-\") and current_entity:\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entity_dict[current_entity_type] = \" \".join(current_entity)\n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "\n",
    "    if current_entity:\n",
    "        entity_dict[current_entity_type] = \" \".join(current_entity)\n",
    "\n",
    "    return entity_dict\n",
    "\n",
    "\n",
    "sample_text = \"\"\"DRIVING LICENCE DEMOCRATIC SOCIALIST REPUBLIC OF SRI LANKA 200035304389 4829786\n",
    "VINNATH NINURA SATHARASINGHE 279/5 RATHMALDENIYA ROAD PANNIPITIYA\n",
    "18.2000 18.2020 18.2028 Blood Group B+ J.M.U.K Jayasekera Commissioner General Motor Traffic\"\"\"\n",
    "\n",
    "predictions_dict = predict_entities_dict(sample_text)\n",
    "\n",
    "print(\"Extracted Entities:\\n\")\n",
    "for entity, value in predictions_dict.items():\n",
    "    print(f\"{entity}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
